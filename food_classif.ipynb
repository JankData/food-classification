{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Si1_GCNC9NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB7ANS7_E0vd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "import os\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "rjEdocDwReAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating functions"
      ],
      "metadata": {
        "id": "vAX6ijVp1LJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders\"\"\"\n",
        "\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "nUQmxGWCiZdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Turns a target model to training mode and runs through the training steps\"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        y_pred = model(X)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "dOJt4v9VnMeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Turns a target model to eval mode and performs a forward pass on a testing dataset\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "vAfilfdBqo67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"Passes a target model through train_step() and test_step()\n",
        "    for a number of epochs training and testing the model\n",
        "    in the same epoch loop.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ImuEZzELrtrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "a2kuva3avNQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(results):\n",
        "    \"\"\"Plots training curves of a results dictionary\"\"\"\n",
        "\n",
        "    loss = results[\"train_loss\"]\n",
        "    test_loss = results[\"test_loss\"]\n",
        "\n",
        "    accuracy = results[\"train_acc\"]\n",
        "    test_accuracy = results[\"test_acc\"]\n",
        "\n",
        "    epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label=\"train_loss\")\n",
        "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "FsQ9JkA0vcAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a model to a target directory (Google Drive)\"\"\"\n",
        "\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \\\n",
        "        \"model_name should end with '.pt' or '.pth'\"\n",
        "\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(), f=model_save_path)"
      ],
      "metadata": {
        "id": "xlhX90W7B8Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "EX0hmcTYv4kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a 20% custom dataset"
      ],
      "metadata": {
        "id": "asD1wiIO2SA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_base_path = pathlib.Path(\"/content/drive/MyDrive/food-classification\")\n",
        "drive_base_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "local_base_path = pathlib.Path(\"/content/data\")"
      ],
      "metadata": {
        "id": "tytyV4I8JsOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.Food101(root=drive_base_path,\n",
        "                              split=\"train\",\n",
        "                              download=True)\n",
        "\n",
        "test_data = datasets.Food101(root=drive_base_path,\n",
        "                             split=\"test\",\n",
        "                             download=True)"
      ],
      "metadata": {
        "id": "LY9CGy1uwF5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "hw6uE3qPy_FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names[:10]"
      ],
      "metadata": {
        "id": "FL1sA7Rk0A20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(class_names[train_data[0][1]])\n",
        "train_data[0][0]"
      ],
      "metadata": {
        "id": "ekQ8soP70EUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "data_path = drive_base_path / \"food-101\" / \"images\"\n",
        "target_classes = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "amount_to_get = 0.2\n",
        "\n",
        "def get_subset(image_path=data_path,\n",
        "               base_path=drive_base_path,\n",
        "               data_splits=[\"train\", \"test\"],\n",
        "               target_classes=[\"pizza\", \"steak\", \"sushi\"],\n",
        "               amount=0.1,\n",
        "               seed=42):\n",
        "    random.seed(seed)\n",
        "    label_splits = {}\n",
        "\n",
        "    for data_split in data_splits:\n",
        "        print(f\"[INFO] Creating image split for: {data_split}...\")\n",
        "        label_path = base_path / \"food-101\" / \"meta\" / f\"{data_split}.txt\"\n",
        "        with open(label_path, \"r\") as f:\n",
        "            labels = [line.strip(\"\\n\") for line in f.readlines() if line.split(\"/\")[0] in target_classes]\n",
        "\n",
        "        number_to_sample = round(amount * len(labels))\n",
        "        print(f\"[INFO] Getting random subset of {number_to_sample} images for {data_split}...\")\n",
        "        sampled_images = random.sample(labels, k=number_to_sample)\n",
        "\n",
        "        image_paths = [pathlib.Path(str(image_path / sample_image) + \".jpg\") for sample_image in sampled_images]\n",
        "        label_splits[data_split] = image_paths\n",
        "    return label_splits\n",
        "\n",
        "label_splits = get_subset(amount=amount_to_get)\n",
        "label_splits[\"train\"][:10]"
      ],
      "metadata": {
        "id": "RUhpWxLE0JXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir_name = drive_base_path / f\"custom_{str(int(amount_to_get*100))}_percent\"\n",
        "print(f\"Creating directory: '{target_dir_name}'\")\n",
        "\n",
        "target_dir = pathlib.Path(target_dir_name)\n",
        "target_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "jCzCRJY72L7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "for image_split in label_splits.keys():\n",
        "    for image_path in label_splits[str(image_split)]:\n",
        "        dest_dir = target_dir / image_split / image_path.parent.stem / image_path.name\n",
        "        if not dest_dir.parent.is_dir():\n",
        "            dest_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(image_path, dest_dir)"
      ],
      "metadata": {
        "id": "SbEKzxV93GLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"Walks through dir_path returning its contents\"\"\"\n",
        "  import os\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "walk_through_dir(target_dir)"
      ],
      "metadata": {
        "id": "MgcQW9iy3fM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_name = drive_base_path / f\"custom_{str(int(amount_to_get*100))}_percent\"\n",
        "\n",
        "shutil.make_archive(zip_file_name,\n",
        "                    format=\"zip\",\n",
        "                    root_dir=target_dir)\n",
        "\n",
        "print(f\"Zip saved to: {zip_file_name}.zip\")"
      ],
      "metadata": {
        "id": "wKl8Jl0o38lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting data"
      ],
      "metadata": {
        "id": "uYogPIOx7o2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = drive_base_path / f\"custom_{str(int(amount_to_get*100))}_percent.zip\"\n",
        "\n",
        "extract_path = local_base_path / f\"custom_{str(int(amount_to_get*100))}_percent\"\n",
        "\n",
        "if not extract_path.exists():\n",
        "    print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "else:\n",
        "    print(\"Data already extracted locally.\")\n",
        "\n",
        "train_dir = extract_path / \"train\"\n",
        "test_dir = extract_path / \"test\"\n",
        "print(f\"Ready for training at: {train_dir}\")"
      ],
      "metadata": {
        "id": "lj5X3jMu4Rlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating an EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "6PibE3J07GQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
        "\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "kc-tTn6Z7zXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "id": "kpuI-4mm8NR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features=1408,\n",
        "              out_features=3))"
      ],
      "metadata": {
        "id": "xwbsiOl78RjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms\"\"\"\n",
        "\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "EnenW-AA-JuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3, seed=42)"
      ],
      "metadata": {
        "id": "vGafF1ap_KI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "dgwj3D6A_RRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "B-Ic0B1T_dEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "set_seeds()\n",
        "effnetb2_results = train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "id": "y6PbtUBWAceZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "id": "CT2bmKA1BA7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(\n",
        "    model=effnetb2,\n",
        "    target_dir=\"/content/drive/MyDrive/food-classification/models\",\n",
        "    model_name=\"pretrained_effnetb2_feature_extractor_custom_20_percent.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ocju9P6eBao8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.load_state_dict(torch.load(\n",
        "    \"/content/drive/MyDrive/food-classification/models/pretrained_effnetb2_feature_extractor_custom_20_percent.pth\"\n",
        "))"
      ],
      "metadata": {
        "id": "C40Of5ueFqCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "zdTKm1T3Bbcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "XTeqb6efFB9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}